---
frameworks:
- Pytorch
license: Apache License 2.0
tasks:
- text-generation
---

# Happy-LLM-215M-Base

<div align='center'>
    <img src="./images/head.jpg" alt="alt text" width="80%">
    <h1>Happy-LLM</h1>
</div>

<div align="center">
  <img src="https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&logo=github" alt="GitHub stars"/>
  <img src="https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&logo=github" alt="GitHub forks"/>
  <img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language"/>
  <a href="https://github.com/datawhalechina/happy-llm"><img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&logo=github" alt="GitHub Project"></a>
  <a href="https://swanlab.cn/@kmno4/Happy-LLM/overview"><img src="https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg" alt="SwanLab"></a>
</div>

&emsp;&emsp;æœ¬æ¨¡å‹ä¸º Happy-LLM 215M åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŸºäº Pytorch æ¡†æ¶ï¼Œæ¨¡å‹å¤§å°ä¸º 215Mï¼Œè®­ç»ƒæ•°æ®ä¸º[å‡ºé—¨é—®é—®åºåˆ—çŒ´å­å¼€æºæ•°æ®é›†](https://www.modelscope.cn/datasets/ddzhu123/seq-monkey/files)ï¼Œæ€»é‡å¤§æ¦‚åœ¨ 10B Token å·¦å³ã€‚

> æ³¨ï¼šæœ¬æ¨¡å‹ä»…ç”¨äºæ•™å­¦ç›®çš„ï¼Œè¯·å‹¿å°†æœ¬æ¨¡å‹åº”ç”¨äºä»»ä½•ç”Ÿäº§ç¯å¢ƒã€‚

> æ³¨ï¼šæ¨¡å‹åœ¨ 8å¡4090 å¹³å°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œbatch_size è®¾ç½®ä¸º 64ï¼Œè®­ç»ƒæ€» step ä¸º 453110ã€‚

## æ¨¡å‹ä¸‹è½½

&emsp;&emsp;å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼ä¸‹è½½æ¨¡å‹

```bash
#å®‰è£…ModelScope
pip install modelscope
```
```python
#SDKæ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('kmno4zx/happy-llm-215M-base')
```

## æ¨¡å‹ä½¿ç”¨

```bash
pip install -r requirements.txt
```

&emsp;&emsp;è¿è¡Œä»¥ä¸‹ä»£ç å³å¯ï¼š

```python
import os
import pickle
from contextlib import nullcontext
import torch
from k_model import ModelConfig, Transformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse

class TextGenerator:
    def __init__(self, 
                 checkpoint='./pretrain_1024_18_6144.pth',  # æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
                 tokenizer_model_path='./tokenizer_k/',  # åˆ†è¯å™¨æ¨¡å‹è·¯å¾„
                 seed=42,  # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
                 device=None,  # è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨ CUDAï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨çš„ CUDAï¼Œåˆ™ä½¿ç”¨ CPU
                 dtype="bfloat16"):  # æ•°æ®ç±»å‹ï¼Œé»˜è®¤ä¸º float32ï¼Œå¯ä»¥é€‰æ‹© float16 æˆ– bfloat16
        """
        åˆå§‹åŒ– TextGenerator ç±»ï¼ŒåŠ è½½æ¨¡å‹ã€è®¾ç½®è®¾å¤‡å’Œåˆ†è¯å™¨ç­‰ã€‚
        """
        # æ¨¡å‹åŠ è½½é…ç½®
        self.checkpoint = checkpoint  # ä¿å­˜çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        self.tokenizer_model_path = tokenizer_model_path  # åˆ†è¯å™¨æ¨¡å‹æ–‡ä»¶è·¯å¾„
        self.seed = seed  # éšæœºæ•°ç§å­ï¼Œç”¨äºç”Ÿæˆçš„å¯é‡å¤æ€§
        self.device = device or ('cuda:0' if torch.cuda.is_available() else 'cpu')  # æ ¹æ®ç¡¬ä»¶æ¡ä»¶é€‰æ‹©è®¾å¤‡
        self.dtype = dtype  # æ¨¡å‹çš„æµ®ç‚¹æ•°ç±»å‹
        self.device_type = 'cuda' if 'cuda' in self.device else 'cpu'  # åˆ¤æ–­å½“å‰è®¾å¤‡æ˜¯å¦ä¸º CUDA
        
        # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç”Ÿæˆçš„å¯é‡å¤æ€§
        torch.manual_seed(seed)  # è®¾ç½® CPU éšæœºç§å­
        torch.cuda.manual_seed(seed)  # è®¾ç½® CUDA éšæœºç§å­
        torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸ CUDA ä½¿ç”¨ TF32 ç²¾åº¦è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—
        torch.backends.cudnn.allow_tf32 = True  # å…è®¸ cuDNN ä½¿ç”¨ TF32 ç²¾åº¦åŠ é€Ÿ
        
        # æ ¹æ® dtype é€‰æ‹©é€‚å½“çš„è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[self.dtype]
        self.ctx = nullcontext() if self.device_type == 'cpu' else torch.amp.autocast(device_type=self.device_type, dtype=ptdtype)
        
        # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_dict = torch.load(self.checkpoint, map_location=self.device)  # åŠ è½½æ¨¡å‹å‚æ•° # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        self.model = Transformer(ModelConfig(dim=1024, n_layers=18))  # å®ä¾‹åŒ– Transformer æ¨¡å‹
        sunwanted_prefix = '_orig_mod.'
        for k, v in list(checkpoint_dict.items()):
            if k.startswith(sunwanted_prefix):
                checkpoint_dict[k[len(sunwanted_prefix):]] = checkpoint_dict.pop(k)
        self.model.load_state_dict(checkpoint_dict, strict=False)
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡
        num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Model has {num_params / 1e6:.3f} M parameters.")
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆevaluation modeï¼‰ï¼Œé˜²æ­¢è®­ç»ƒæ¨¡å¼ä¸‹çš„ dropout ç­‰æ“ä½œå½±å“ç»“æœ
        self.model.eval()
        # å°†æ¨¡å‹æ”¾ç½®åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆGPU æˆ– CPUï¼‰
        self.model.to(self.device)
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_model_path)  # æ ¹æ®æŒ‡å®šçš„è·¯å¾„åŠ è½½åˆ†è¯å™¨

    def pretrain_sample(self, 
               start="Hello!",  # ç”Ÿæˆæ–‡æœ¬çš„èµ·å§‹æç¤ºè¯ï¼Œå¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ä¸²
               num_samples=3,  # ç”Ÿæˆæ ·æœ¬çš„æ•°é‡ï¼Œé»˜è®¤ç”Ÿæˆ 3 ä¸ªæ ·æœ¬
               max_new_tokens=256,  # æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼Œé»˜è®¤æœ€å¤šç”Ÿæˆ 256 ä¸ª token
               temperature=0.7,  # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œ1.0 ä¸ºæ ‡å‡†ï¼Œå€¼è¶Šå¤§è¶Šéšæœº
               top_k=300):  # ä¿ç•™æ¦‚ç‡æœ€é«˜çš„ top_k ä¸ª tokenï¼Œé™åˆ¶ç”Ÿæˆæ—¶çš„é€‰æ‹©èŒƒå›´
        """
        æ ¹æ®ç»™å®šçš„èµ·å§‹æ–‡æœ¬ç”Ÿæˆæ ·æœ¬ã€‚
        
        :param start: ç”Ÿæˆæ–‡æœ¬çš„èµ·å§‹æç¤ºè¯
        :param num_samples: è¦ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬æ•°
        :param max_new_tokens: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„æœ€å¤§ token æ•°
        :param temperature: æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œå€¼è¶Šå°ç”Ÿæˆè¶Šç¡®å®šï¼Œå€¼è¶Šå¤§ç”Ÿæˆè¶Šéšæœº
        :param top_k: é™åˆ¶ç”Ÿæˆæ—¶é€‰æ‹©çš„ token èŒƒå›´
        :return: ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
        """
        # å¦‚æœ start æ˜¯ä»¥ 'FILE:' å¼€å¤´ï¼Œè¡¨ç¤ºä»æ–‡ä»¶ä¸­è¯»å–èµ·å§‹æ–‡æœ¬
        if start.startswith('FILE:'):
            with open(start[5:], 'r', encoding='utf-8') as f:
                start = f.read()  # è¯»å–æ–‡ä»¶å†…å®¹ä½œä¸ºèµ·å§‹æ–‡æœ¬
        
        # å°†èµ·å§‹æ–‡æœ¬ç¼–ç ä¸º token id åºåˆ—
        start_ids = self.tokenizer(start).data['input_ids']
        # print('start_ids:', start_ids)
        x = (torch.tensor(start_ids, dtype=torch.long, device=self.device)[None, ...])  # å°†ç¼–ç åçš„ token id è½¬ä¸º PyTorch å¼ é‡
        # print(x.shape)
        generated_texts = []  # ç”¨äºä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œæå‡æ•ˆç‡
            with self.ctx:  # è¿›å…¥è‡ªåŠ¨æ··åˆç²¾åº¦çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæ˜¯ GPU å¹¶ä½¿ç”¨ float16 æ—¶ï¼‰
                for k in range(num_samples):  # å¾ªç¯ç”ŸæˆæŒ‡å®šæ•°é‡çš„æ ·æœ¬
                    y = self.model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)  # ç”Ÿæˆæ–‡æœ¬
                    generated_texts.append(self.tokenizer.decode(y[0].tolist()))  # è§£ç ç”Ÿæˆçš„ token åºåˆ—ä¸ºå¯è¯»æ–‡æœ¬
        
        return generated_texts  # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬
    
if __name__ == "__main__":
    print("------------------- Pretrain Sample ------------------- \n")

    pretrain_prompt_datas = [
        '<|im_start|>åŒ—äº¬å¤§å­¦æ˜¯',
        '<|im_start|>ä¸­å›½çŸ¿ä¸šå¤§å­¦ï¼ˆåŒ—äº¬ï¼‰åœ°çƒç§‘å­¦ä¸æµ‹ç»˜å·¥ç¨‹å­¦é™¢',
    ]

    generator = TextGenerator(checkpoint='./base_model_215M/pretrain_1024_18_6144.pth')  # åˆå§‹åŒ–ç”Ÿæˆå™¨
    for i in range(len(pretrain_prompt_datas)):
        samples = generator.pretrain_sample(start=pretrain_prompt_datas[i], num_samples=1, max_new_tokens=120, temperature=0.75)
        print(f"\nSample {i+1}:\n{pretrain_prompt_datas[i]}{samples[0]}\n{'-'*20}")  # æ‰“å°ç”Ÿæˆçš„æ ·æœ¬å¹¶ç”¨åˆ†éš”çº¿åˆ†å‰²
```

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [å®‹å¿—å­¦-é¡¹ç›®è´Ÿè´£äºº](https://github.com/KMnO4-zx) (Datawhaleæˆå‘˜-ä¸­å›½çŸ¿ä¸šå¤§å­¦(åŒ—äº¬))
- [é‚¹é›¨è¡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/logan-zou) (Datawhaleæˆå‘˜-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/)ï¼ˆDatawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆï¼‰

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

<div align=center style="margin-top: 30px;">
  <a href="https://github.com/datawhalechina/happy-llm/graphs/contributors">
    <img src="https://contrib.rocks/image?repo=datawhalechina/happy-llm" />
  </a>
</div>

## Star History

<div align='center'>
    <img src="./images/star-history-2025612.png" alt="Datawhale" width="90%">
</div>

<div align="center">
  <p>â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼</p>
</div>

## å…³äº Datawhale

<div align='center'>
    <img src="./images/datawhale.png" alt="Datawhale" width="30%">
    <p>æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹</p>
</div>
